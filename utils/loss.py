import torch
import torch.nn as nn
import torch.nn.functional as F
import math


# ====================================================================================
# ç¬¬ä¸€é˜¶æ®µï¼šå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆTriplet Lossï¼‰
# ====================================================================================
class TripletLoss(nn.Module):
    """
    ä¸‰å…ƒç»„æŸå¤± - ç”¨äº Stage1 é¢„è®­ç»ƒ
    æ‹‰è¿‘ anchor å’Œ positiveï¼Œæ¨è¿œ anchor å’Œ negative
    """
    def __init__(self, margin=0.5, mining='hard'):
        super(TripletLoss, self).__init__()
        self.margin = margin
        self.mining = mining  # 'none', 'hard', 'semi-hard'

    def forward(self, feat_anchor, feat_positive, feat_negative):
        # L2 å½’ä¸€åŒ–
        feat_anchor = F.normalize(feat_anchor, p=2, dim=1)
        feat_positive = F.normalize(feat_positive, p=2, dim=1)
        feat_negative = F.normalize(feat_negative, p=2, dim=1)

        # è®¡ç®—ä½™å¼¦è·ç¦»ï¼ˆ1 - cosine_similarityï¼‰
        dist_ap = 1.0 - F.cosine_similarity(feat_anchor, feat_positive, dim=1)
        dist_an = 1.0 - F.cosine_similarity(feat_anchor, feat_negative, dim=1)

        # Triplet loss with margin
        triplet_loss = F.relu(dist_ap - dist_an + self.margin)

        # Hard miningï¼ˆå¯é€‰ï¼‰
        if self.mining == 'hard':
            # åªä¿ç•™å›°éš¾æ ·æœ¬ï¼ˆloss > 0ï¼‰
            hard_triplets = triplet_loss > 0
            if hard_triplets.sum() > 0:
                triplet_loss = triplet_loss[hard_triplets].mean()
            else:
                triplet_loss = triplet_loss.mean()
        else:
            triplet_loss = triplet_loss.mean()

        return triplet_loss, dist_ap.mean(), dist_an.mean()


class IdentityLoss(nn.Module):
    """
    Identity Loss - ç”¨äº Stage1
    é€šè¿‡ç®€å•çš„åˆ†ç±»ä»»åŠ¡å¢å¼ºç‰¹å¾çš„åˆ¤åˆ«æ€§
    """
    def __init__(self, feat_dim, num_classes, s=16.0):
        super(IdentityLoss, self).__init__()
        self.feat_dim = feat_dim
        self.num_classes = num_classes
        self.s = s
        self.weight = nn.Parameter(torch.FloatTensor(num_classes, feat_dim))
        nn.init.xavier_uniform_(self.weight)

    def forward(self, features, labels):
        """
        Args:
            features: (N, feat_dim) ç‰¹å¾å‘é‡
            labels: (N,) ç±»åˆ«æ ‡ç­¾
        """
        # L2 å½’ä¸€åŒ–
        features_norm = F.normalize(features, dim=1)
        weight_norm = F.normalize(self.weight, dim=0)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¹¶ç¼©æ”¾
        logits = F.linear(features_norm, weight_norm) * self.s

        # äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(logits, labels)
        return loss, logits


class Stage1Loss(nn.Module):
    """
    Stage1 å®Œæ•´æŸå¤±ï¼šTriplet Loss + Identity Loss
    æ ¹æ®è®ºæ–‡å›¾ç‰‡è®¾è®¡
    """
    def __init__(self, feat_dim, num_classes,
                 triplet_margin=0.5,
                 triplet_mining='hard',
                 w_triplet=1.0,
                 w_identity=0.5,
                 identity_s=16.0):
        super(Stage1Loss, self).__init__()
        self.w_triplet = w_triplet
        self.w_identity = w_identity

        self.triplet_loss = TripletLoss(margin=triplet_margin, mining=triplet_mining)
        self.identity_loss = IdentityLoss(feat_dim, num_classes, s=identity_s)

    def forward(self, feat_anchor, feat_positive, feat_negative, labels):
        """
        Args:
            feat_anchor: (N, feat_dim) anchorç‰¹å¾
            feat_positive: (N, feat_dim) positiveç‰¹å¾
            feat_negative: (N, feat_dim) negativeç‰¹å¾
            labels: (N,) anchorçš„ç±»åˆ«æ ‡ç­¾
        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: å„é¡¹æŸå¤±çš„å­—å…¸
        """
        loss_dict = {}

        # 1. Triplet Loss
        triplet_loss, dist_ap, dist_an = self.triplet_loss(feat_anchor, feat_positive, feat_negative)
        loss_dict['triplet'] = triplet_loss.item()
        loss_dict['dist_ap'] = dist_ap.item()
        loss_dict['dist_an'] = dist_an.item()

        # 2. Identity Lossï¼ˆä»…å¯¹ anchor è®¡ç®—ï¼‰
        identity_loss, logits = self.identity_loss(feat_anchor, labels)
        loss_dict['identity'] = identity_loss.item()

        # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
        _, pred = torch.max(logits, 1)
        acc = (pred == labels).float().mean()
        loss_dict['identity_acc'] = acc.item()

        # æ€»æŸå¤±
        total_loss = self.w_triplet * triplet_loss + self.w_identity * identity_loss
        loss_dict['total'] = total_loss.item()

        return total_loss, loss_dict


# ====================================================================================
# ç¬¬äºŒé˜¶æ®µï¼šå¤šä»»åŠ¡æŸå¤±ï¼ˆé€‚é… Stage2FusionCAï¼‰
# ====================================================================================

class ArcFaceLoss(nn.Module):
    """
    ArcFace Lossï¼ˆå·²é›†æˆåœ¨ Stage2FusionCA ä¸­ï¼‰
    è¿™é‡Œæä¾›ç‹¬ç«‹ç‰ˆæœ¬ä»¥ä¾¿çµæ´»ä½¿ç”¨
    """
    def __init__(self, in_features, out_features, s=64.0, m=0.50, easy_margin=False):
        super(ArcFaceLoss, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

        self.easy_margin = easy_margin
        self.cos_m = math.cos(m)
        self.sin_m = math.sin(m)
        self.th = math.cos(math.pi - m)
        self.mm = math.sin(math.pi - m) * m

    def forward(self, input, label):
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        sine = torch.sqrt(torch.clamp(1.0 - cosine**2, min=1e-9))
        phi = cosine * self.cos_m - sine * self.sin_m

        if self.easy_margin:
            phi = torch.where(cosine > 0, phi, cosine)
        else:
            phi = torch.where(cosine > self.th, phi, cosine - self.mm)

        one_hot = torch.zeros_like(cosine)
        one_hot.scatter_(1, label.view(-1, 1), 1.0)
        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)
        output *= self.s
        return output


class ModalityConsistencyLoss(nn.Module):
    """
    æ¨¡æ€ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿èåˆç‰¹å¾ä¿ç•™ä¸¤ä¸ªæ¨¡æ€çš„ä¿¡æ¯
    é€šè¿‡å¯¹æ¯”å…¨å±€ç‰¹å¾å’Œå±€éƒ¨ç‰¹å¾çš„èåˆç»“æœ
    """
    def __init__(self, temperature=0.07):
        super(ModalityConsistencyLoss, self).__init__()
        self.temperature = temperature

    def forward(self, global_fused, local_fused):
        """
        Args:
            global_fused: å…¨å±€ç‰¹å¾èåˆç»“æœ (N, dim)
            local_fused: å±€éƒ¨ç‰¹å¾èåˆç»“æœ (N, dim)
        """
        # L2 å½’ä¸€åŒ–
        global_fused = F.normalize(global_fused, dim=1)
        local_fused = F.normalize(local_fused, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = torch.matmul(global_fused, local_fused.t()) / self.temperature

        # å¯¹è§’çº¿åº”è¯¥æ˜¯æœ€å¤§çš„ï¼ˆåŒä¸€æ ·æœ¬çš„å…¨å±€å’Œå±€éƒ¨åº”è¯¥ä¸€è‡´ï¼‰
        labels = torch.arange(global_fused.size(0), device=global_fused.device)
        loss = F.cross_entropy(logits, labels)

        return loss


class AttentionRegularizationLoss(nn.Module):
    """
    æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤± - é¼“åŠ±æ³¨æ„åŠ›æƒé‡çš„å¤šæ ·æ€§
    é¿å…æ³¨æ„åŠ›æƒé‡é€€åŒ–ï¼ˆæ€»æ˜¯åå‘æŸä¸€ä¸ªæ¨¡æ€ï¼‰
    """
    def __init__(self, epsilon=1e-8):
        super(AttentionRegularizationLoss, self).__init__()
        self.epsilon = epsilon

    def forward(self, attention_weights_palm, attention_weights_vein):
        """
        Args:
            attention_weights_palm: æŒçº¹æ³¨æ„åŠ›æƒé‡ (N, C) æˆ– (N, 1, H, W)
            attention_weights_vein: æŒé™è„‰æ³¨æ„åŠ›æƒé‡ (N, C) æˆ– (N, 1, H, W)
        """
        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›æƒé‡
        mean_palm = attention_weights_palm.mean()
        mean_vein = attention_weights_vein.mean()

        # é¼“åŠ±æƒé‡æ¥è¿‘ 0.5ï¼ˆä¸¤ä¸ªæ¨¡æ€å¹³è¡¡ï¼‰
        balance_loss = (mean_palm - 0.5) ** 2 + (mean_vein - 0.5) ** 2

        # é¼“åŠ±æƒé‡æœ‰ä¸€å®šçš„æ–¹å·®ï¼ˆé¿å…æ‰€æœ‰ä½ç½®æƒé‡ç›¸åŒï¼‰
        var_palm = attention_weights_palm.var()
        var_vein = attention_weights_vein.var()
        diversity_loss = torch.exp(-var_palm) + torch.exp(-var_vein)

        return balance_loss + 0.1 * diversity_loss


class CenterLoss(nn.Module):
    """
    Center Loss - æ‹‰è¿‘åŒç±»æ ·æœ¬çš„ç‰¹å¾
    é…åˆ ArcFace ä½¿ç”¨å¯ä»¥è¿›ä¸€æ­¥æå‡ç±»å†…ç´§å‡‘æ€§
    """
    def __init__(self, num_classes, feat_dim, lambda_c=0.01):
        super(CenterLoss, self).__init__()
        self.num_classes = num_classes
        self.feat_dim = feat_dim
        self.lambda_c = lambda_c

        # å¯å­¦ä¹ çš„ç±»ä¸­å¿ƒ
        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))
        nn.init.xavier_uniform_(self.centers)

    def forward(self, features, labels):
        """
        Args:
            features: ç‰¹å¾å‘é‡ (N, feat_dim)
            labels: ç±»åˆ«æ ‡ç­¾ (N,)
        """
        # L2 å½’ä¸€åŒ–
        features = F.normalize(features, dim=1)
        centers = F.normalize(self.centers, dim=1)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åˆ°å…¶ç±»ä¸­å¿ƒçš„è·ç¦»
        batch_size = features.size(0)
        centers_batch = centers[labels]
        loss = ((features - centers_batch) ** 2).sum() / batch_size

        return self.lambda_c * loss


class Stage2FusionLoss(nn.Module):
    """
    Stage2 å®Œæ•´æŸå¤±å‡½æ•° - é€‚é… Stage2FusionCA æ¶æ„

    åŒ…å«ä»¥ä¸‹æŸå¤±é¡¹ï¼š
    1. åˆ†ç±»æŸå¤±ï¼ˆCrossEntropy æˆ– ArcFaceï¼‰
    2. æ¨¡æ€ä¸€è‡´æ€§æŸå¤±ï¼ˆå¯é€‰ï¼‰
    3. æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±ï¼ˆå¯é€‰ï¼‰
    4. Center Lossï¼ˆå¯é€‰ï¼‰

    æ¨èé…ç½®ï¼š
    - ä»…åˆ†ç±»: w_cls=1.0, å…¶ä½™ä¸º 0
    - æ ‡å‡†é…ç½®: w_cls=1.0, w_consistency=0.1, w_attention=0.05
    - å®Œæ•´é…ç½®: w_cls=1.0, w_consistency=0.1, w_attention=0.05, w_center=0.01
    """
    def __init__(self,
                 num_classes,
                 feat_dim=512,
                 w_cls=1.0,
                 w_consistency=0.0,
                 w_attention=0.0,
                 w_center=0.0,
                 label_smoothing=0.1,
                 use_arcface=True):
        super(Stage2FusionLoss, self).__init__()

        self.w_cls = w_cls
        self.w_consistency = w_consistency
        self.w_attention = w_attention
        self.w_center = w_center
        self.use_arcface = use_arcface

        # 1. åˆ†ç±»æŸå¤±ï¼ˆå¦‚æœ Stage2FusionCA ä¸­å·²åŒ…å« ArcFaceï¼Œæ­¤å¤„ä½¿ç”¨ CEï¼‰
        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)

        # 2. æ¨¡æ€ä¸€è‡´æ€§æŸå¤±
        if w_consistency > 0:
            self.consistency_loss = ModalityConsistencyLoss(temperature=0.07)

        # 3. æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±
        if w_attention > 0:
            self.attention_loss = AttentionRegularizationLoss()

        # 4. Center Loss
        if w_center > 0:
            self.center_loss = CenterLoss(num_classes, feat_dim, lambda_c=1.0)

    def forward(self, logits, labels, fused_feat=None, details=None):
        """
        Args:
            logits: åˆ†ç±» logits (N, num_classes)
            labels: æ ‡ç­¾ (N,)
            fused_feat: èåˆåçš„ç‰¹å¾å‘é‡ (N, feat_dim)ï¼Œç”¨äº Center Loss
            details: Stage2FusionCA è¿”å›çš„ä¸­é—´ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - global['fused']: å…¨å±€èåˆç‰¹å¾ (N, G)
                - local['fused']: å±€éƒ¨èåˆç‰¹å¾ (N, L)
                - global['w_palm'], global['w_vein']: å…¨å±€æ³¨æ„åŠ›æƒé‡
                - local['w_palm'], local['w_vein']: å±€éƒ¨æ³¨æ„åŠ›æƒé‡
        """
        loss_dict = {}

        # 1. åˆ†ç±»æŸå¤±
        cls_loss = self.ce_loss(logits, labels)
        loss_dict['cls'] = cls_loss.item()
        total_loss = self.w_cls * cls_loss

        # 2. æ¨¡æ€ä¸€è‡´æ€§æŸå¤±
        if self.w_consistency > 0 and details is not None:
            global_fused = details['global']['fused']
            local_fused = details['local']['fused']
            consistency_loss = self.consistency_loss(global_fused, local_fused)
            loss_dict['consistency'] = consistency_loss.item()
            total_loss += self.w_consistency * consistency_loss

        # 3. æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±
        if self.w_attention > 0 and details is not None:
            # å…¨å±€æ³¨æ„åŠ›æ­£åˆ™åŒ–
            global_attn_loss = self.attention_loss(
                details['global']['w_palm'],
                details['global']['w_vein']
            )
            # å±€éƒ¨æ³¨æ„åŠ›æ­£åˆ™åŒ–
            local_attn_loss = self.attention_loss(
                details['local']['w_palm'],
                details['local']['w_vein']
            )
            attention_loss = (global_attn_loss + local_attn_loss) / 2.0
            loss_dict['attention'] = attention_loss.item()
            total_loss += self.w_attention * attention_loss

        # 4. Center Loss
        if self.w_center > 0 and fused_feat is not None:
            center_loss = self.center_loss(fused_feat, labels)
            loss_dict['center'] = center_loss.item()
            total_loss += self.w_center * center_loss

        loss_dict['total'] = total_loss.item()
        return total_loss, loss_dict


# ====================================================================================
# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºæ¨èçš„æŸå¤±å‡½æ•°
# ====================================================================================

def get_stage1_loss(feat_dim, num_classes,
                    triplet_margin=0.5, triplet_mining='hard',
                    w_triplet=1.0, w_identity=0.5,
                    identity_s=16.0):
    """
    è·å– Stage1 æ¨èæŸå¤±å‡½æ•°ï¼ˆTriplet Loss + Identity Lossï¼‰

    Args:
        feat_dim: ç‰¹å¾ç»´åº¦ï¼ˆViT: 192, CNN: 768ï¼‰
        num_classes: ç±»åˆ«æ•°ï¼ˆç”¨äº Identity Lossï¼‰
        triplet_margin: Triplet marginï¼ˆå»ºè®® 0.3-1.0ï¼‰
        triplet_mining: å›°éš¾æ ·æœ¬æŒ–æ˜ç­–ç•¥ ('none', 'hard')
        w_triplet: Triplet Lossæƒé‡
        w_identity: Identity Lossæƒé‡
        identity_s: Identity Lossçš„ç¼©æ”¾å› å­
    """
    return Stage1Loss(
        feat_dim=feat_dim,
        num_classes=num_classes,
        triplet_margin=triplet_margin,
        triplet_mining=triplet_mining,
        w_triplet=w_triplet,
        w_identity=w_identity,
        identity_s=identity_s
    )


def get_stage2_loss(num_classes, feat_dim=512, mode='standard'):
    """
    è·å– Stage2 æ¨èæŸå¤±å‡½æ•°

    Args:
        num_classes: ç±»åˆ«æ•°
        feat_dim: ç‰¹å¾ç»´åº¦ï¼ˆout_dim_global + out_dim_localï¼‰
        mode: æŸå¤±æ¨¡å¼
            - 'simple': ä»…åˆ†ç±»æŸå¤±
            - 'standard': åˆ†ç±» + ä¸€è‡´æ€§
            - 'full': åˆ†ç±» + ä¸€è‡´æ€§ + æ³¨æ„åŠ›æ­£åˆ™åŒ–
            - 'advanced': å®Œæ•´é…ç½®ï¼ˆåŒ…å« Center Lossï¼‰
    """
    if mode == 'simple':
        return Stage2FusionLoss(
            num_classes=num_classes,
            feat_dim=feat_dim,
            w_cls=1.0,
            w_consistency=0.0,
            w_attention=0.0,
            w_center=0.0,
            label_smoothing=0.0  # ğŸ”§ å…³é—­ label smoothing
        )
    elif mode == 'standard':
        return Stage2FusionLoss(
            num_classes=num_classes,
            feat_dim=feat_dim,
            w_cls=1.0,
            w_consistency=0.1,
            w_attention=0.0,
            w_center=0.0,
            label_smoothing=0.1
        )
    elif mode == 'full':
        return Stage2FusionLoss(
            num_classes=num_classes,
            feat_dim=feat_dim,
            w_cls=1.0,
            w_consistency=0.1,
            w_attention=0.05,
            w_center=0.0,
            label_smoothing=0.1
        )
    elif mode == 'advanced':
        return Stage2FusionLoss(
            num_classes=num_classes,
            feat_dim=feat_dim,
            w_cls=1.0,
            w_consistency=0.1,
            w_attention=0.05,
            w_center=0.01,
            label_smoothing=0.1
        )
    else:
        raise ValueError(f"Unknown mode: {mode}. Choose from ['simple', 'standard', 'full', 'advanced']")
